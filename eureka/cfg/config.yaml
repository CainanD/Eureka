defaults:
  - _self_
  - env: shadow_hand
  - override hydra/launcher: local
  - override hydra/output: local

hydra:
  job:
    chdir: True

# LLM parameters
# model: gpt-4.1-mini  # LLM model (other options are gpt-4, gpt-4-0613, gpt-3.5-turbo-16k-0613)
model: gpt-4.1-mini
temperature: 1.0
suffix: GPT  # suffix for generated files (indicates LLM model)

# Eureka parameters
sample: 2 # number of reward functions GPT generates per iteration
training_mode: individual # "individual" (train n separate policies) or "averaged" (train 1 policy with averaged reward)
vlm_iterations: 1 # number of VLM feedback loops (default=1 means 2 total GPT generations)
max_iterations: 500 # RL Policy training iterations (decrease this to make the feedback loop faster)
capture_video: True # whether to capture policy rollout videos
manual_success: False # whether to use manual success function 

checkpoint: "" # path to checkpoint to load (if any)

# Weights and Biases
use_wandb: False # whether to use wandb for logging
wandb_username: "" # wandb username if logging with wandb
wandb_project: "" # wandb project if logging with wandb